# -*- coding: utf-8 -*-
"""linguistic analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZaoKTNwHk5jOX-cwqU-cQOp1u0fyIlc
"""

import pandas as pd
import nltk
import spacy
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from nltk.sentiment import SentimentIntensityAnalyzer
from gensim.models import TfidfModel
from gensim.corpora import Dictionary
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from spacy import displacy

# Download NLTK resources
nltk.download('punkt')
nltk.download('vader_lexicon')
nltk.download('stopwords')

# Load the data
#cola_text = pd.read_csv('cola.tsv')
humor_text = pd.read_excel('AI_Humor.xlsx')
# humor_n_text = pd.read_csv('n.csv')
# humor_p_text = pd.read_csv('p.csv')
# # sarcasmNew_text = pd.read_csv('sacarsmNew.csv')
# # sentenceNew_text = pd.read_csv('sentenceNew.csv')

# humor_n_text = humor_n_text.rename(columns={'text': 'Text'})
# humor_p_text = humor_p_text.rename(columns={'text': 'Text'})
# humor_text = humor_text.rename(columns={'text': 'Text'})
# #imbdNew_text = imbdNew_text.rename(columns={'review': 'Text'})
# # sarcasmNew_text = sarcasmNew_text.rename(columns={'headlines': 'Text'})
# # sentenceNew_text = sentenceNew_text.rename(columns={'sentence': 'Text'})

humor_text

humor_n_text

sarcasmNew_text

sentenceNew_text

# Preprocessing function
def preprocess(text):
    tokens = nltk.word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalnum()]
    return tokens

# Extract features and visualize
def analyze_data(df, label):
    # Tokenize the text
    df['tokens'] = df['headlines'].apply(preprocess)

    # Calculate Type-Token Ratio (TTR)
    total_tokens = sum(df['tokens'].apply(len))
    unique_tokens = len(set(word for words in df['tokens'] for word in words))
    TTR = unique_tokens / total_tokens
    print(f"{label} - Type-Token Ratio (TTR): {TTR}")

    # N-gram frequency distribution
    ngrams = Counter(ngram for tokens in df['tokens'] for ngram in nltk.ngrams(tokens, 2))
    ngrams = ngrams.most_common(10)

    plt.figure()
    plt.barh(range(len(ngrams)), [count for _, count in ngrams], align='center')
    plt.yticks(range(len(ngrams)), [ngram for ngram, _ in ngrams])
    plt.xlabel('Frequency')
    plt.title(f"{label} - Top 10 Bigrams")
    plt.show()

# POS tagging
    nlp = spacy.load("en_core_web_sm")
    pos_counts = Counter()
    for doc in nlp.pipe(df['Text'].astype(str), batch_size=50, n_process=-1):
        pos_counts.update([token.pos_ for token in doc])

    # Sort POS tags by frequency
    sorted_pos_counts = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)

    plt.figure()
    plt.barh(range(len(sorted_pos_counts)), [count for _, count in sorted_pos_counts], align='center')
    plt.yticks(range(len(sorted_pos_counts)), [pos for pos, _ in sorted_pos_counts])
    plt.xlabel('Frequency')
    plt.ylabel('POS')
    plt.title(f"{label} - POS Distribution")
    plt.gca().invert_yaxis()  # Invert y-axis to display higher values at the top
    plt.show()

# Analyze and visualize AI-generated text
analyze_data(humor_text, "Text")
# analyze_data(humor_n_text, "humor_n")
# analyze_data(humor_p_text, "humor_p")
#analyze_data(sarcasmNew_text, "sarcasmNew")
#analyze_data(sentenceNew_text, "sentenceNew")

#Long-range dependencies (average sentence length)

nltk.download('punkt')

def avg_sentence_length(text):
    sentences = nltk.sent_tokenize(text)
    return sum(len(nltk.word_tokenize(sentence)) for sentence in sentences) / len(sentences)

humorNew_text['avg_sentence_length'] = humorNew_text['Text'].apply(avg_sentence_length)
imbdNew_text['avg_sentence_length'] = humorNew_text['Text'].apply(avg_sentence_length)
sarcasmNew_text['avg_sentence_length'] = sarcasmNew_text['Text'].apply(avg_sentence_length)
sentenceNew_text['avg_sentence_length'] = sentenceNew_text['Text'].apply(avg_sentence_length)

humorNew_text

imbdNew_text

sarcasmNew_text

sentenceNew_text

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Combine all human-generated text into a single string
humorNew_text_combined = ' '.join(humorNew_text['Text'])
# Generate a word cloud for human-generated text
humorNew_wordcloud = WordCloud(width=800, height=500, background_color='white', stopwords=STOPWORDS).generate(humorNew_text_combined)

# Combine all AI-generated text into a single string
imbdNew_text_combined = ' '.join(imbdNew_text['Text'])
# Generate a word cloud for AI-generated text
imbdNew_wordcloud = WordCloud(width=800, height=500, background_color='white', stopwords=STOPWORDS).generate(imbdNew_text_combined)

# Combine all AI-generated text into a single string
sarcasmNew_text_combined = ' '.join(sarcasmNew_text['Text'])
# Generate a word cloud for AI-generated text
sarcasmNew_wordcloud = WordCloud(width=800, height=500, background_color='white', stopwords=STOPWORDS).generate(sarcasmNew_text_combined)

# Combine all AI-generated text into a single string
sentenceNew_text_combined = ' '.join(sentenceNew_text['Text'])
# Generate a word cloud for AI-generated text
sentenceNew_wordcloud = WordCloud(width=800, height=500, background_color='white', stopwords=STOPWORDS).generate(sentenceNew_text_combined)

# Plot the word clouds side by side
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.imshow(humorNew_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('humorNew')

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 2)
plt.imshow(imbdNew_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('imbdNew')
plt.show()

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 2)
plt.imshow(sarcasmNew_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('sarcasmNew')
plt.show()

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 2)
plt.imshow(sentenceNew_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('sentenceNew')
plt.show()