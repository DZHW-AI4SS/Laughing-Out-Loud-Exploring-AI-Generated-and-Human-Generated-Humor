# -*- coding: utf-8 -*-
"""Humor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gQ6OG0D7tK2boGw8x8hCFPkOx-EsNrjl
"""

!pip install transformers

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

data = pd.read_excel('Human&AI_generated_Humor_Detection.xlsx')
data

# for index, row in data.iterrows():
#     if row['Label'] == 1:
#         Humor.append(row)

train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

class HumorDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]['Text']
        label = self.data.iloc[idx]['generated_label'] #adjust  for the respective label!
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long),
        }

tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

train_dataset = HumorDataset(train_data, tokenizer, max_length=128)
val_dataset = HumorDataset(val_data, tokenizer, max_length=128)

test_data= pd.read_excel('humor_01.xlsx')

len(test_data)

len(arr)

arr=[]
for x in test_data.Text:
  arr.append(0)

test_data['generated_label']=arr

test_data

#test_data= pd.read_excel('AI_or_Human_Humor_test.xlsx')
test_dataset = HumorDataset(test_data, tokenizer, max_length=128)

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_dataloader =  DataLoader(test_dataset, batch_size=16, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(train_dataloader) * 3  # Assuming 3 epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

model.to(device)

#This includes evaluation metrics (accuracy, F1 score, precision, and recall), model checkpointing based on the best F1 score, and early stopping if the validation F1 score does not improve for a specified number of epochs (3 in this case).


from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

def compute_metrics(labels, preds):
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='weighted')
    precision = precision_score(labels, preds, average='weighted')
    recall = recall_score(labels, preds, average='weighted')
    return {"accuracy": accuracy, "f1": f1, "precision": precision, "recall": recall}

def train(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_train_loss = 0
    all_labels = []
    all_preds = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        model.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_train_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        preds = torch.argmax(outputs.logits, dim=1)
        all_labels.extend(labels.cpu().tolist())
        all_preds.extend(preds.cpu().tolist())

    avg_train_loss = total_train_loss / len(dataloader)
    metrics = compute_metrics(all_labels, all_preds)
    return avg_train_loss, metrics

def evaluate(model, dataloader, device):
    model.eval()
    total_eval_loss = 0
    all_labels = []
    all_preds = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

        loss = outputs.loss
        total_eval_loss += loss.item()

        preds = torch.argmax(outputs.logits, dim=1)
        all_labels.extend(labels.cpu().tolist())
        all_preds.extend(preds.cpu().tolist())

    avg_eval_loss = total_eval_loss / len(dataloader)
    metrics = compute_metrics(all_labels, all_preds)
    return avg_eval_loss, metrics

epochs = 3
best_f1 = 0
early_stopping_counter = 0
early_stopping_limit = 5

for epoch in range(epochs):
    print(f'Epoch {epoch + 1}/{epochs}')
    print('-' * 30)

    train_loss, train_metrics = train(model, train_dataloader, optimizer, scheduler, device)
    print(f'Train loss: {train_loss}')
    print(f'Train metrics: {train_metrics}')

    eval_loss, eval_metrics = evaluate(model, val_dataloader, device)
    print(f'Validation loss: {eval_loss}')
    print(f'Validation metrics: {eval_metrics}')

     # Save the model with the best F1 score
    if eval_metrics['f1'] > best_f1:
        best_f1 = eval_metrics['f1']
        model.save_pretrained('humor_detection_roberta_best')
        early_stopping_counter = 0
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= early_stopping_limit:
            print(f'Early stopping after {early_stopping_counter} epochs without improvement')
            break

"""The results show that the fine-tuned RoBERTa model is performing well on both the training and validation sets. The training loss decreases, and the training metrics (accuracy, F1 score, precision, and recall) are consistently high, indicating that the model is learning from the training data. The validation metrics are also high, which means that the model generalizes well to the unseen data.

However, the validation metrics remain the same across all three epochs. This could be due to the model converging quickly, the learning rate being too low, or the dataset being relatively small and easy for the model.

It is essential to evaluate the model on a separate test set to understand its true performance on unseen data. Additionally, experimenting with different hyperparameters (e.g., learning rate, number of epochs, batch size) and using techniques like cross-validation can help in finding the best configuration for your specific problem.
"""

def visualize_classification(model, dataloader, device):
    model.eval()

    true_positives = []
    true_negatives = []
    false_positives = []
    false_negatives = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)

        preds = torch.argmax(outputs.logits, dim=1)

        for label, pred, input_id in zip(labels, preds, input_ids):
            if label == 1 and pred == 1:
                true_positives.append(tokenizer.decode(input_id))
            elif label == 0 and pred == 0:
                true_negatives.append(tokenizer.decode(input_id))
            elif label == 0 and pred == 1:
                false_positives.append(tokenizer.decode(input_id))
            elif label == 1 and pred == 0:
                false_negatives.append(tokenizer.decode(input_id))

    return true_positives, true_negatives, false_positives, false_negatives

true_positives, true_negatives, false_positives, false_negatives = visualize_classification(model, val_dataloader, device)

print("True Positives:")
for tp in true_positives:
    print(tp)

print("\nTrue Negatives:")
for tn in true_negatives:
    print(tn)

print("\nFalse Positives:")
for fp in false_positives:
    print(fp)

print("\nFalse Negatives:")
for fn in false_negatives:
    print(fn)

import pandas as pd

def create_classification_table(model, dataloader, device):
    model.eval()

    texts = []
    true_labels = []
    predicted_labels = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)

        preds = torch.argmax(outputs.logits, dim=1)

        for label, pred, input_id in zip(labels, preds, input_ids):
            texts.append(tokenizer.decode(input_id))
            true_labels.append(label.item())
            predicted_labels.append(pred.item())

    classification_table = pd.DataFrame({
        'text': texts,
        'True Label': true_labels,
        'Predicted Label': predicted_labels
    })

    return classification_table

classification_table = create_classification_table(model, test_dataloader, device)
print(classification_table)

classification_table.to_excel('result.xlsx', index=False)

# Download the file in Google Colab
from google.colab import files
files.download('result.xlsx')

model.save_pretrained("path/to/trained/model")

# Load the saved model from a file
loaded_model = RobertaForSequenceClassification.from_pretrained("path/to/trained/model")

# test_dataset = HumorDataset(test_data, tokenizer, max_length=128)
# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=2)

# # Move the test dataset to the same device as the model
# test_dataset = HumorDataset(
#     test_data,
#     tokenizer=tokenizer,
#     max_length=128,
# )
# # Move the test dataset to the same device as the model
# for i in range(len(test_dataset)):
#     test_dataset[i]['input_ids'] = test_dataset[i]['input_ids'].to(device)
#     test_dataset[i]['attention_mask'] = test_dataset[i]['attention_mask'].to(device)
#     test_dataset[i]['label'] = test_dataset[i]['label'].to(device)

# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=2)

test_loss, test_metrics = evaluate(model, test_dataloader, device)
print(f'Test loss: {test_loss}')
print(f'Test metrics: {test_metrics}')

# for i in range(len(test_dataset)):
#     test_dataset[i]['input_ids'] = test_dataset[i]['input_ids'].to(device)
#     test_dataset[i]['attention_mask'] = test_dataset[i]['attention_mask'].to(device)
#     test_dataset[i]['label'] = test_dataset[i]['label'].to(device)

# test_loss, test_metrics = evaluate(loaded_model, test_dataloader, device)
# print(f'Test loss: {test_loss}')
# print(f'Test metrics: {test_metrics}')

test_loss, test_metrics = evaluate(loaded_model, test_dataloader, device)
print(f'Test loss: {test_loss}')
print(f'Test metrics: {test_metrics}')